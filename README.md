# NLP Dersi: LoRA ile Fine-Tuning Projesi (Proje 2)

Bu proje, **Qwen2.5-Coder-1.5B** modeli Ã¼zerinde LoRA (Low-Rank Adaptation) yÃ¶ntemi kullanÄ±larak gerÃ§ekleÅŸtirilen bir ince ayar (fine-tuning) Ã§alÄ±ÅŸmasÄ±nÄ± ve bu sÃ¼reÃ§teki teknik Ã§Ä±karÄ±mlarÄ± iÃ§ermektedir.

## Proje Ã–zeti

* **Base Model:** `Qwen2.5-Coder-1.5B-Instruct` (1.5B parametre)
* **YÃ¶ntem:** LoRA Fine-Tuning
* **Veri Setleri (Datasets):**
    * `Naholav/CodeGen-Diverse-5K`
    * `Naholav/CodeGen-Deep-5K`
* **EÄŸitim AlanlarÄ± (Training Fields):** `input`, `solution` (Sadece kod odaklÄ±, reasoning/muhakeme iÃ§ermeyen yapÄ±)


## ğŸ› ï¸ KullanÄ±lan Hiperparametreler ve Ã–ÄŸrendiklerim

### 1. Max Sequence Length (`max_seq_len`)
* **Ä°ÅŸlevi:** Modelin "attention" matrisinin boyutunu ve tek seferde okuyabileceÄŸi metin uzunluÄŸunu belirler (Context Window).
* **Etkisi:** Modelin aynÄ± anda kaÃ§ token gÃ¶rebileceÄŸini ve baÄŸlam kurabileceÄŸini tanÄ±mlar.
* **DonanÄ±m Ä°liÅŸkisi:** Bu parametrenin deÄŸeri **VRAM** kullanÄ±mÄ± ile doÄŸru orantÄ±lÄ±dÄ±r. AyrÄ±ca verilecek eÄŸitim verisinin uzunluÄŸu ile uyumlu olmalÄ±dÄ±r.

### 2. Learning Rate (`lr`)
* **Ä°ÅŸlevi:** Modelin her adÄ±mda aÄŸÄ±rlÄ±klarÄ± ne kadar deÄŸiÅŸtireceÄŸini belirler. LoRA matrislerinin (A ve B) gÃ¼ncellenme hÄ±zÄ±dÄ±r.
* **Denge:**
    * **YÃ¼ksek LR:** Model hÄ±zlÄ± Ã¶ÄŸrenir ancak kararsÄ±z (unstable) hale gelebilir ve Ã§Ä±ktÄ±larÄ± bozulabilir.
    * **DÃ¼ÅŸÃ¼k LR:** Model daha istikrarlÄ± Ã¶ÄŸrenir, genel performans artar ve *overfit* riski azalÄ±r; ancak eÄŸitim sÃ¼resi Ã§ok uzayabilir.
    * **Ã–zetle:** Ã‡ok bÃ¼yÃ¼k olursa model bozulur, Ã§ok kÃ¼Ã§Ã¼k olursa model Ã¶ÄŸrenemez.

### 3. Rank (`r`)
* **Ä°ÅŸlevi:** LoRA'nÄ±n eklediÄŸi matrislerin boyutunu belirler. Bilginin ne kadar detaylÄ± kodlanacaÄŸÄ±nÄ± temsil eder.
* **Etkisi:**
    * **BÃ¼yÃ¼k R:** LoRA daha fazla bilgiyi encode eder, ana model daha fazla deÄŸiÅŸime uÄŸrar.
    * **KÃ¼Ã§Ã¼k R:** Model Ã¼zerinde Ã§ok sÄ±nÄ±rlÄ± deÄŸiÅŸiklik yapar.
* **DonanÄ±m Ä°liÅŸkisi:** **VRAM** kullanÄ±mÄ± ile birebir iliÅŸkilidir. R deÄŸeri bÃ¼yÃ¼dÃ¼kÃ§e parametre sayÄ±sÄ± artacaÄŸÄ± iÃ§in eÄŸitim hÄ±zÄ± yavaÅŸlar, azaldÄ±kÃ§a hÄ±z artar.

### 4. Alpha (`lora_alpha`)
* **Ä°ÅŸlevi:** LoRA gÃ¼ncellemelerinin temel modele ne kadarlÄ±k bir Ã¶lÃ§ekte etki edeceÄŸini belirleyen katsayÄ±dÄ±r (GÃ¼ncelleme gÃ¼Ã§ seviyesi).
* **Denge:**
    * **BÃ¼yÃ¼k Alpha:** Model daha hÄ±zlÄ± Ã¶ÄŸrenir ancak *overfitting* riski artar.
    * **KÃ¼Ã§Ã¼k Alpha:** *Overfitting* riski azalÄ±r ancak model domain bilgisini (yeni veriyi) yeterince Ã¶ÄŸrenemeyebilir.

### 5. Checkpoints
* **Ä°ÅŸlevi:** EÄŸitim sÄ±rasÄ±nda belirli adÄ±mlarda modelin LoRA aÄŸÄ±rlÄ±klarÄ±nÄ±n ve optimizer durumunun kaydedilmesidir.
* **AvantajÄ±:** Bir versiyonlama sistemi gibi Ã§alÄ±ÅŸÄ±r. GPU kesintisi veya teknik aksaklÄ±klarda eÄŸitimin kaybedilmemesini (yedekleme) saÄŸlar ve en iyi performans veren adÄ±mÄ±n seÃ§ilmesine olanak tanÄ±r.

### 6. Epoch
* **Ä°ÅŸlevi:** EÄŸitim verilerinin tamamÄ±nÄ±n modelden kaÃ§ kez geÃ§irileceÄŸini ifade eder.
* **Ã–rnek:** 10.000 verilik bir set ve 10 epoch iÃ§in toplamda 100.000 training step gerÃ§ekleÅŸir.
* **Etkisi:**
    * **Fazla Epoch:** Model veriyi ezberlemeye baÅŸlar (*overfitting*).
    * **Az Epoch:** Model veriyi tam Ã¶ÄŸrenemez (*underfitting*).
* **Not:** Epoch sayÄ±sÄ± eÄŸitim sÃ¼resini doÄŸrudan etkiler ancak VRAM kullanÄ±mÄ±nÄ± etkilemez.
